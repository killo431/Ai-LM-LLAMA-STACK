# === General Settings ===
RESTART_OPTION=unless-stopped
COMPOSE_PROJECT_NAME=ai-stack

# === Database Configuration ===
POSTGRES_USER=admin
POSTGRES_PASSWORD=Texas123!
POSTGRES_DB=ai_stack

# === CrewAI Studio Settings ===
# Web interface for no-code CrewAI agent creation
CREWAI_STUDIO_PORT=8501
#OLLAMA_HOST=http://localhost:11434

# === Flowise Settings ===
# Visual workflow builder for LLMs
FLOWISE_USERNAME=admin
FLOWISE_PASSWORD=admin

# === LightRAG Settings ===
# Knowledge graph RAG system
LIGHTRAG_PORT=9621

# === Open WebUI Settings ===
# ChatGPT-style interface for local models
OPEN_WEBUI_PORT=8080
OPENWEBUI_DOCKER_TAG=main
OPENWEBUI_AUTH=false

# === n8n Workflow Automation ===
N8N_ENCRYPTION_KEY=n8n-encryption-key-change-this-in-production
WEBHOOK_URL=http://localhost:5678

# === AMD GPU Configuration ===
# For Ollama ROCm support
ROCM_VERSION=6.0
HSA_OVERRIDE_GFX_VERSION=
HIP_VISIBLE_DEVICES=0

# === API Keys ===
# Replace with your actual API keys

# OpenAI API (can remain set but is unused when LLM_PROVIDER=lmstudio)
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Anthropic Claude API (optional)
# ANTHROPIC_API_KEY=sk-ant-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Groq API (optional - fast inference)
# GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# Search API (optional - for web search capabilities)
# SERPER_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

# === LLM Provider Selection ===
# Force the app to use LM Studio's OpenAI-compatible server
LLM_PROVIDER=lmstudio

# LM Studio server (containers -> host bridge). Ensure LM Studio Local Server is running on port 1234.
LMSTUDIO_API_BASE=http://host.docker.internal:1234/v1
# Any non-empty key; some clients require an Authorization header even if LM Studio ignores it
LMSTUDIO_API_KEY=lm-studio

# === Default Model Configuration ===
# Use the EXACT model id shown by: curl -sS http://host.docker.internal:1234/v1/models
# Example below is based on your logs; replace if LM Studio shows a different id.
DEFAULT_CHAT_MODEL=Deepseek-R1-0528-Qwen3-8B
DEFAULT_EMBEDDING_MODEL=nomic-embed-text
DEFAULT_CODE_MODEL=codellama:7b

# === Service URLs (Internal Docker Network) ===
# These are used for inter-container communication
OLLAMA_INTERNAL_URL=http://ollama:11434
QDRANT_INTERNAL_URL=http://qdrant:6333
POSTGRES_INTERNAL_URL=postgres:5432

# === External Access URLs ===
# These are for accessing services from your host machine
OLLAMA_EXTERNAL_URL=http://localhost:11434
QDRANT_EXTERNAL_URL=http://localhost:6333
POSTGRES_EXTERNAL_URL=localhost:5432

# === Resource Limits ===
# Adjust based on your system capabilities
MEMORY_LIMIT=16G
CPU_CORES=8

# === Security Settings ===
# Set to true in production environments
SECURE_MODE=false
ENABLE_AUTH=false

# === Logging & Telemetry Configuration ===
LOG_LEVEL=INFO
ENABLE_TELEMETRY=false

# Strongly reduce 3rd-party telemetry/noise in logs
OTEL_SDK_DISABLED=true
OTEL_TRACES_EXPORTER=none
CREWAI_TELEMETRY_OPT_OUT=1
POSTHOG_DISABLED=true
PYTHONWARNINGS=ignore::pydantic.warnings.PydanticDeprecatedSince20,ignore::DeprecationWarning:posthog.*

# GPU device groups (fill these with the numeric values from Step 1)
VIDEO_GID=44         # example only; replace with your host's value
RENDER_GID=107       # example only; replace with your host's value
KFD_GID=302          # example only; replace with your host's value (ROCm)