[
  {
    "name": "Qwen3 4B Function Calling Pro",
    "model_family": "qwen3",
    "quantization": "GGUF Q4_K_M",
    "size": "3.99 GB",
    "context_length": 4096,
    "source": "Manojb",
    "description": "Qwen3 4B optimized for function calling and structured tasks.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 1024,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "auto",
      "v_cache_quantization_type": "auto"
    }
  },
  {
    "name": "Magistral Small 2509",
    "model_family": "llama",
    "quantization": "GGUF Q4_K_M",
    "size": "14.17 GB",
    "context_length": 16384,
    "source": "mistralai",
    "description": "24B parameter Llama-based model for advanced reasoning.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 256,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "q4",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "Deepseek R1 0528 Qwen3 8B",
    "model_family": "qwen3",
    "quantization": "GGUF Q4_K_M",
    "size": "4.68 GB",
    "context_length": 8192,
    "source": "deepseek",
    "description": "Qwen3 8B fine-tuned by Deepseek for general reasoning tasks.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 512,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "fp16",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "Unsloth",
    "model_family": "llama",
    "quantization": "GGUF Q8_0",
    "size": "3.78 GB",
    "context_length": 8192,
    "source": "Chituyi",
    "description": "Efficient 7B model designed for performance and accuracy.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 512,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "fp16",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "Zephyr 7B Sft Full SPIN Iter3 I1",
    "model_family": "llama",
    "quantization": "GGUF Q4_K_S",
    "size": "3.86 GB",
    "context_length": 8192,
    "source": "mradermacher",
    "description": "Zephyr conversational model tuned for natural dialogue.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 512,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "fp16",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "E5 Large v2",
    "model_family": "bert",
    "quantization": "GGUF Q8_0",
    "size": "341.52 MB",
    "context_length": 4096,
    "source": "mradermacher",
    "description": "High-quality embedding model for semantic search and RAG.",
    "default_params": {
      "temperature": 0.0,
      "top_p": 1.0,
      "gpu_layers": 0,
      "max_tokens": 512,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 2048,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": false,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": false,
      "k_cache_quantization_type": "q4",
      "v_cache_quantization_type": "q4"
    }
  },
  {
    "name": "T3Q Qwen2.5 14B v1.0 E3",
    "model_family": "qwen2",
    "quantization": "GGUF Q4_K_M",
    "size": "8.37 GB",
    "context_length": 16384,
    "source": "tensorblock",
    "description": "14B Qwen2.5 tuned for advanced reasoning and long context tasks.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 256,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "q4",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "Nomic Embed Text v2 MoE",
    "model_family": "nomic-bert-moe",
    "quantization": "GGUF Q6_K",
    "size": "378.75 MB",
    "context_length": 4096,
    "source": "nomic-ai",
    "description": "Mixture-of-experts embedding model optimized for scalability.",
    "default_params": {
      "temperature": 0.0,
      "top_p": 1.0,
      "gpu_layers": 0,
      "max_tokens": 512,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 2048,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": false,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": false,
      "k_cache_quantization_type": "q4",
      "v_cache_quantization_type": "q4"
    }
  },
  {
    "name": "Llama 3 8B Instruct Abliterated v2",
    "model_family": "llama",
    "quantization": "GGUF Q6_K",
    "size": "6.14 GB",
    "context_length": 8192,
    "source": "mradermacher",
    "description": "Optimized Llama 3 8B instruct model for reasoning and general chat.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 512,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "fp16",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "Qwen3 Zro Cdr Reason v2 0.8B NEO EX D AU",
    "model_family": "qwen3",
    "quantization": "GGUF F16",
    "size": "1.53 GB",
    "context_length": 4096,
    "source": "DavidAU",
    "description": "Lightweight Qwen3 reasoning model, ideal for utility and small tasks.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 16,
      "max_tokens": 1024,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 1024,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "auto",
      "v_cache_quantization_type": "auto"
    }
  },
  {
    "name": "Mistral 7B Instruct v0.3",
    "model_family": "llama",
    "quantization": "GGUF Q4_K_M",
    "size": "4.07 GB",
    "context_length": 8192,
    "source": "mistralai",
    "description": "Mistral 7B instruct-tuned model for fast and efficient reasoning.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 512,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "fp16",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "OpenAI's gpt-oss 20B",
    "model_family": "gpt-oss",
    "quantization": "GGUF MXFP4",
    "size": "11.28 GB",
    "context_length": 16384,
    "source": "openai",
    "description": "20B parameter open-source GPT model for advanced reasoning.",
    "default_params": {
      "temperature": 0.7,
      "top_p": 0.9,
      "gpu_layers": 32,
      "max_tokens": 2048,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 256,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": true,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": true,
      "k_cache_quantization_type": "q4",
      "v_cache_quantization_type": "fp16"
    }
  },
  {
    "name": "Nomic Embed Text v1.5",
    "model_family": "bert",
    "quantization": "GGUF Q4_K_M",
    "size": "lightweight",
    "context_length": 4096,
    "source": "nomic-ai",
    "description": "Efficient embedding model for RAG pipelines and search.",
    "default_params": {
      "temperature": 0.0,
      "top_p": 1.0,
      "gpu_layers": 0,
      "max_tokens": 512,
      "cpu_thread_pool_size": 6,
      "evaluation_batch_size": 2048,
      "rope_freq_base": 10000,
      "rope_freq_scale": 1.0,
      "offload_kv_cache_to_gpu": false,
      "keep_model_in_memory": true,
      "try_mmap": true,
      "seed": "auto",
      "flash_attention": false,
      "k_cache_quantization_type": "q4",
      "v_cache_quantization_type": "q4"
    }
  }
]